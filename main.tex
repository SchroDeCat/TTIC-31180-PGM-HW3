%!TEX program = pdflatex
% \documentclass[11pt,en]{elegantpaper}

\title{HW-3}
\author{Fengxue Zhang}

\date{\today}
\documentclass{article}
\usepackage{algpseudocode}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}

%Import the natbib package and sets a bibliography  and citation styles
\usepackage{natbib}
\bibliographystyle{abbrvnat}

% global notations
\newcommand{\NonNegativeReals}{\ensuremath{\mathbb{R}_{\ge 0}}}
\newcommand{\PositiveIntegers}{\ensuremath{\mathbb{Z^+}}}
\newcommand{\integers}{\ensuremath{\mathbb{Z}}}
\newcommand{\nats}{\ensuremath{\mathbb{N}}}
\newcommand{\reals}{\ensuremath{\mathbb{R}}}
\newcommand{\rationals}{\ensuremath{\mathbb{Q}}}
\newcommand{\distrib}[0]{\ensuremath{\mathcal{D}}}
\newcommand{\matroid}{\ensuremath{\mathcal{M}}}
\newcommand{\model}[0]{\ensuremath{\mathcal{M}}}
\newcommand{\hypotheses}[0]{\mathcal{H}}
\newcommand{\hvar}[0]{\ensuremath{H}}
\newcommand{\hypothesis}[0]{\ensuremath{h}}

\newcommand{\instance}[0]{\ensuremath{\mathbf{x}}}
\newcommand{\latentrep}[0]{\ensuremath{\mathbf{z}}}
\newcommand{\GramMat}[0]{\ensuremath{\mathbf{K}}}
\newcommand{\Selected}[0]{\ensuremath{\mathbf{A}}}
\newcommand{\DataSet}[0]{\ensuremath{\mathbf{D}}}
\newcommand{\LatentRepSet}[0]{\ensuremath{\mathbf{Z}}}

\newcommand{\secref}[1]{\S\ref{#1}}
\begin{document}

\maketitle

\section*{Q1}
\subsection*{(a)}
\begin{equation}
    \begin{split}
        \min_{Q} D(P\Vert Q) &= \min_{Q}\sum_{X, Y} P(X, Y) \log\frac{P(X, Y)}{Q(X, Y)}\\
                    &= \min_{Q}\sum_{X, Y} P(X, Y) \log\frac{P(X, Y)}{Q(X)Q(Y)}\\
    \end{split}
\end{equation} 
Then we see that minimizing the KL divergence is equivalent to maximize the following formula.
\begin{equation} \label{eq-KL-M}
    \begin{split}
                    &arg\max_{Q}\sum_{X, Y} P(X, Y) (\log Q(X) + \log Q(Y))  \\
                    &= arg\max_{Q}\sum_{X, Y} P(X, Y) \log Q(X) + \sum_{X, Y} P(X, Y) \log Q(Y)\\
                    &= arg\max_{Q}\sum_{X} P(X) \log Q(X) + \sum_{Y} P(Y) \log Q(Y)\\
                    &= arg\min_{Q} \sum_{X} P(X) \log 1/Q(X) + \sum_{Y} P(Y) \log 1/Q(Y)
    \end{split}
\end{equation}
The last line of \eqref{eq-KL-M} is equivalent to minimize the single variate KL divergence respectively.
According to the minimization of single variate KL divergence, we know that the original KL-divergence is minimized when $P(Y) = Q(Y)$ and $P(X) = Q(X)$. 


\subsection*{(b)}
We consider the expected likelihood.
\begin{equation}
    \begin{split}
        arg\max_{\theta}E_{X_i \sim P}[\prod_{i=1}^{M}Q(X^{i};\theta)]
        &= arg\max_{\theta}E_{X_i \sim P}[\log \prod_{i=1}^{M}Q(X^{i};\theta)]\\
        &= arg\max_{\theta}E_{X_i \sim P}[\sum_{i=1}^{M}\log Q(X^{i};\theta)] \\
        &= arg\max_{\theta} \sum_{i=1}^{M}P(X_i)\log Q(X^{i};\theta)\\
        &= arg\min_{\theta} \sum_{i=1}^{M}P(X_i)\log P(X_i)\\
        &\quad -\sum_{i=1}^{M}P(X_i)\log Q(X^{i};\theta)\\
        &= arg\min_{\theta} \sum_{i=1}^{M}P(X_i)\log \frac{P(X_i)}{Q(X^{i};\theta)}\\
        &= arg\min_{\theta} D(P\Vert Q)
    \end{split}
\end{equation}

\subsection*{(c)}
\begin{equation}
    \begin{split}
        \min_{Q} D(Q\Vert P) &= \min_{Q}\sum_{X, Y} Q(X, Y) \log\frac{Q(X, Y)}{P(X, Y)}\\
                    &= \min_{Q}\sum_{X, Y} {Q(X)Q(Y)} \log\frac{Q(X)Q(Y)}{P(X, Y)}\\
                    &= \min_{Q}\sum_{X, Y} {Q(X)Q(Y)} \log\frac{Q(X)Q(Y)}{P(X, Y)}\\
                    &= \min_{Q}\sum_{X} {Q(X)\log Q(X)} + \sum_{Y} {Q(Y)\log Q(Y)}\\
                    &\quad  - \sum_{X, Y} {Q(X)Q(Y)} \log{P(X, Y)}\\
                    &= \min_{Q} [-H_Q(X) - H_Q(Y) + CrossEntropy_P(Q(X,Y))]
    \end{split}
\end{equation}
With constraints that $\sum_X{Q(X)}=1$ and $\sum_Y{Q(Y)}=1$.
Then applying lagrangian methods:
\begin{equation}
    \begin{split}
        \min_{Q} D(Q\Vert P)
                    &= \min_{Q}\sum_{X} {Q(X)\log Q(X)} + \sum_{Y} {Q(Y)\log Q(Y)}\\
                    &\quad - \sum_{X, Y} {Q(X)Q(Y)} \log{P(X, Y)} \\
                    &\quad + \lambda_1 (\sum_X{Q(X)} - 1) + \lambda_2 (\sum_Y{Q(Y)} - 1)\\
                    &= \min_Q f(X, Y)
    \end{split}
\end{equation}
The corresponding partial derativies are 
$$\frac{\partial{f}}{\partial{Q(X_i)}} = \log Q(X_i) + 1 + \lambda_1 - \sum_Y Q(Y)\log P(X_i, Y) = 0$$
$$\frac{\partial{f}}{\partial{Q(Y_i)}} = \log Q(Y_i) + 1 + \lambda_2 - \sum_X Q(X)\log P(X, Y_i) = 0$$
$$\frac{\partial{f}}{\partial\lambda_1} = \sum_X{Q(X)}-1 = 0$$ 
$$\frac{\partial{f}}{\partial\lambda_2} = \sum_Y{Q(Y)}-1 = 0$$

\noindent Since we know whenever $P(X, Y) = 0 \rightarrow Q(X, Y) = 0$, then we have
$$\frac{\partial{f}}{\partial{Q(X_1)}} = \log Q(X_1) + 1 + \lambda_1 -  \log(\frac{1}{8})( Q(Y_1) +  Q(Y_2))= 0$$
$$\frac{\partial{f}}{\partial{Q(X_2)}} = \log Q(X_2) + 1 + \lambda_1 -  \log(\frac{1}{8})( Q(Y_1) +  Q(Y_2))= 0$$
$$\frac{\partial{f}}{\partial{Q(X_3)}} = \log Q(X_3) + 1 + \lambda_1 -  \log(\frac{1}{4})( Q(Y_3))= 0$$
$$\frac{\partial{f}}{\partial{Q(X_4)}} = \log Q(X_4) + 1 + \lambda_1 -  \log(\frac{1}{4})( Q(Y_4))= 0$$
The results are symmetic for $\frac{\partial{f}}{\partial{Q(Y_i)}}$. Then we get $X_1 = X_2$, and $Y_1 = Y_2$. 
Also we found that $Q(X_3)$ and $Q(X_4)$ can not be non-zero at the same time, otherwise $Q(Y_1) = Q(Y_2) = Q(Y_3) = Q(Y_4) = 0$. 
Symmetrically, $Q(Y_3)$ and $Q(Y_4)$ can not be non-zero at the same time.

\noindent (1) When $Q(X_3) = Q(X_4) = Q(Y_3) = Q(Y_4) = 0$, $Q(X_1) = Q(X_2) = Q(Y_1) = Q(Y_2) = \frac{1}{2}$.
$D(Q\Vert P) = 4 * \frac{1}{4}\log \frac{\frac{1}{4}}{\frac{1}{8}} = \log 2$.

\noindent (2) When $Q(X_4)\neq0$, we have $Q(X_3) = Q(Y_1) = Q(Y_2) = Q(Y_3) = 0$, $Q(Y_4) = 1$ then $Q(X_1) = Q(X_2) = Q(X_3) = 0$ and $Q(X_4) = 1$.
$D(Q\Vert P) = \log \frac{1}{\frac{1}{4}} = 2\log 2$.

\noindent (3) Symmetrically, when $Q(X_x)\neq0$, we have $Q(X_4) = Q(Y_1) = Q(Y_2) = Q(Y_4) = 0$, $Q(Y_3) = 1$ then $Q(X_1) = Q(X_2) = Q(X_4) = 0$ and $Q(X_4) = 1$.
$D(Q\Vert P) = \log \frac{1}{\frac{1}{4}} = 2\log 2$.

\noindent (4) When $Q(X, Y) = P(X)P(Y) = \frac{1}{4}\frac{1}{4}$, we don't have $D(Q||P)$ as when $P(X, Y) = 0$, $Q(X, Y) = P(X) P(Y) > 0$.

\section*{Q2}
\subsection*{(a)}
\begin{proof}
    \textbf{Firstly}, we prove that any member of the marginal polytope of the clique tree is in the local consistency polytope.
    By the corollary 10.2 in the textbook $\beta_i(C_i)=\sum_{\mathcal{X}-C_i}\tilde{P}_{\phi}(\mathcal{X})$, 
    the marginal distribution meets the consistency requirement, thus $\forall \mu_Q \in M, \mu_Q \in Local(\mathcal{U})$.
    \textbf{Secondly}, we prove that any member of the local consistency polytope for a clique tree is in the marginal polytope. By theorem 10.4 in the textbook,
    for the calibrated potentials for the clique tree $\mathcal{T}$, since $\tilde{P}_{\phi}\propto Q_{\mathcal{T}}$, we have $\beta_i(c_i) \propto \tilde{P}_{\phi}(c_i)$.
    By definition $\sum_{c_i}\beta_i(c_i)=1$, then we get $\beta_i(c_i) = \tilde{P}_{\phi}(c_i)$. In conclusion, for any clique tree, the local consistency polytope is equal to the marginal
    polytope.
\end{proof}

\subsection*{(b)}
\begin{proof}
    Consider a cluster graph containing three nodes: $C_1 = {A, B}$, $C_2 = {A, D}$, and $C_3 = {B, D}$. Let $A \in \{0, 1\}$, $B = A, D = 1 - A$.
    Then there exists $\mu_Q \in Local(\mathcal{U})$, such that $\beta_1(A=1, B=0)=\beta_1(A=0, B=1)=\frac{1}{2}$, $\beta_2(A=1, D=1)=\beta_2(A=0, D=0)=\frac{1}{2}$, 
    and $\beta_3(B=1, D=1)=\beta_3(B=0, D=0)=\frac{1}{2}$. These believies meet the consistency requirement that (1) the supset believies agree ($\mu(A = 1) = \mu(A = 0) = \mu(B = 1) = \mu(B = 0) = \mu(D = 1) = \mu(D = 0) = \frac{1}{2}$), (2) non-negative, (3) sum is 1. 
    Yet contradict on $C_3$ as it suggests $B = D$ which contradict with believies on $C_1$ and $C_2$.
    Therefore the marginal polytope is \textbf{strictly contained} in the local consistency polytope.
\end{proof}

\section*{Q3}
\subsection*{(a)}
\subsubsection{(i)}
\begin{proof}
    Obviously, the believies meet the (1) non-negative requirement and (2) the normalization requirement ($\sum_{c_i}\beta_i(c_i) = 1, i=1,2,3$). We also have that the (3) supset believies agrees 
    ($\mu_{1,2} (X_2) = \mu_{1,3} (X_1) = \mu_{2,3} (X_3) = (0.5,0.5)$). Therefore it defines a locally consistent pesudo-marginal.
        %    $$ \mu_{1,2} (X_2 = 1) = \sum_{X_1}(\beta_1(X_1, X_2=1)) = \sum_{X_3}(\beta_2(X_3, X_2=1)) = 0.5 $$
        %    $$\mu_{1,2} (X_2 = 0) = \sum_{X_1}(\beta_1(X_1, X_2=0)) = \sum_{X_3}(\beta_2(X_3, X_2=0)) = 0.5 $$
        %    $$ \mu_{2,3} (X_3 = 1) = \sum_{X_2}(\beta_2(X_1, X_2=1)) = \sum_{X_1}(\beta_2(X_3, X_2=1)) = 0.5 $$
        %    $$\mu_{2,3} (X_3 = 0) = \sum_{X_2}(\beta_1(X_1, X_2=0)) = \sum_{X_1}(\beta_2(X_3, X_2=0)) = 0.5 $$
\end{proof}

\subsubsection*{(ii)}
\begin{proof}
    If the locally consistent pesudo-margianl constitue a probability distribution, then we have 
    $$P(X_1, X_2, X_3) = \frac{\beta_1(X_1, X_2,) \beta_2(X_3, X_2) \beta_3(X_3, X_1) }{\mu_{1,2} (X_2) \mu_{2,3} (X_3) \mu_{1,3} (X_1)}$$
    $$P(X_1=0, X_2=0, X_3=0) = \frac{0.4 \times 0.4 \times 0.1}{0.5 \times 0.5 \times 0.5} = 0.128$$
    $$P(X_1=0, X_2=0, X_3=1) = \frac{0.4 \times 0.1 \times 0.4}{0.5 \times 0.5 \times 0.5} = 0.128$$
    $$P(X_1=0, X_2=1, X_3=0) = \frac{0.1 \times 0.1 \times 0.1}{0.5 \times 0.5 \times 0.5} = 0.008$$  
    $$P(X_1=1, X_2=0, X_3=0) = \frac{0.1 \times 0.4 \times 0.4}{0.5 \times 0.5 \times 0.5} = 0.128$$
    $$P(X_1=0, X_2=1, X_3=1) = \frac{0.1 \times 0.4 \times 0.4}{0.5 \times 0.5 \times 0.5} = 0.128$$
    $$P(X_1=1, X_2=0, X_3=1) = \frac{0.1 \times 0.1 \times 0.1}{0.5 \times 0.5 \times 0.5} = 0.008$$
    $$P(X_1=1, X_2=1, X_3=0) = \frac{0.4 \times 0.1 \times 0.4}{0.5 \times 0.5 \times 0.5} = 0.128$$
    $$P(X_1=1, X_2=1, X_3=1) = \frac{0.4 \times 0.4 \times 0.1}{0.5 \times 0.5 \times 0.5} = 0.128$$
    Then $$\sum P(X_1, X_2, X_3) = 0.784 < 1$$ \noindent It's not a valid distribution.
    % $$P(X_1=1, X_2=0, X_3=0) = \frac{0.1 \times 0.4 \times 0.4}{0.5 \times 0.5} = 0.064$$
\end{proof}

\subsection*{(b)}
From $P_{\mathcal{T}}(A,B,C,D) = P_{\Phi}(A,B,C,D)\frac{\mu_{3,4}(D)\mu_{1,4}(A)}{\beta_4(A,D)}$, we have
\begin{equation}
    \begin{split}
        \beta_1(A,B) &= P_{\mathcal{T}}(A,B)\\
                & = \sum_{C,D}P_{\mathcal{T}}(A,B,C,D)\\
                % & = \sum_{C,D}P_{\Phi}(A,B,C,D)\frac{\mu_{1,4}(A)\mu_{1,2}(B)}{\beta_1(A,B)}\\
                % & = r(A,B)P_{\Phi}(A,B)
                & = \sum_{C,D}P_{\Phi}(A,B,C,D)\frac{\mu_{3,4}(D)\mu_{1,4}(A)}{\beta_4(A,D)}\\
                & = \sum_{D}P_{\Phi}(A,B,D)\frac{\mu_{3,4}(D)\mu_{1,4}(A)}{\beta_4(A,D)}\\
                & = \sum_{D}P_{\Phi}(A,B,D)\frac{\mu_{3,4}(D)\mu_{1,4}(A)}{\beta_1(A,B)}\\
                & \leq \frac{\mu_{1,4}(A)}{\beta_1(A,B)} (\sum_{D}P_{\Phi}(A,B,D))(\max_{D}\mu_{3,4}(D))\\
                & = P_{\Phi}(A,B)\sum_{D}\frac{\mu_{3,4}(D)\mu_{1,4}(A)}{\beta_1(A,B)}\\
                & = P_{\Phi}(A,B)\sum_{D}\frac{\mu_{3,4}(D)\mu_{1,4}(A)}{\beta_4(A,D)}\\
                & = P_{\Phi}(A,B)\sum_{D} r(A,D)
    \end{split}
\end{equation}
Here we use the fact that $\frac{\mu_{1,4}(A)}{\beta_1(A,B)}= \frac{\mu_{1,4}(A)}{\beta_4(A,D)}$ in the calibrated graph and $\mu_{3,4}(D) > 0$
to derive the final inequality $\beta_1(A,B) \leq P_{\Phi}(A,B)\max_{D} r(A,D)$. Similarly, we could derive $\beta_1(A,B) \geq P_{\Phi}(A,B)\min_{D} r(A,D)$.


\section*{Q4}

\end{document}
\end{proof}



\section*{Q4}

\end{document}